{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof of Concept Strawman\n",
    "\n",
    "Here we demonstrate how using simple methods like multiple gradient boosted trees can help achieve our goals. Inspiration taken from Talos and the UCLNLP group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import word_tokenize\n",
    "from helpers import preprocess_data\n",
    "from ngram import getBigram, getTrigram, getFourgram\n",
    "from featureExtractors import CountFeatureGenerator, TfidfFeatureGenerator\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, precision_recall_curve, plot_precision_recall_curve\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CACHE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to read from the cache\n",
    "if USE_CACHE:\n",
    "    with open(\"df.pickle\", \"rb\") as handle:\n",
    "        training_data = pickle.loads(handle.read())\n",
    "    print(f\"Training Data Size: {len(training_data)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading entire dataset into a single dataframe\n",
    "if not USE_CACHE:\n",
    "    bodies = pd.read_csv(\"train_bodies.csv\").set_index('Body ID')\n",
    "    training_data = pd.read_csv(\"train_stances.csv\")\n",
    "    training_data['head'] = training_data['Headline']\n",
    "    training_data['body'] = bodies['articleBody'][training_data['Body ID']].reset_index(drop=True)\n",
    "    del training_data['Body ID']\n",
    "    del training_data['Headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out all \"unrelated/discuss items\"\n",
    "if not USE_CACHE:\n",
    "    training_data = training_data[training_data['Stance'] != 'unrelated']\n",
    "    training_data = training_data[training_data['Stance'] != 'discuss']\n",
    "    training_data['Stance'] = training_data['Stance'] == 'agree'\n",
    "    training_data['Stance'] = training_data['Stance'].astype(int)\n",
    "    training_data = training_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tokenize the respective contents\n",
    "if not USE_CACHE:\n",
    "    training_data['body_tokens'] = training_data.apply(lambda row: word_tokenize(row['body'].lower()), axis=1)\n",
    "    training_data['head_tokens'] = training_data.apply(lambda row: word_tokenize(row['head'].lower()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create n-grams\n",
    "if not USE_CACHE:\n",
    "    training_data['body_unigrams'] = training_data.apply(lambda row: preprocess_data(row['body']), axis=1)\n",
    "    training_data['body_bigrams'] = training_data.apply(lambda row: getBigram(row['body_unigrams'], '_'), axis=1)\n",
    "    training_data['body_trigrams'] = training_data.apply(lambda row: getTrigram(row['body_unigrams'], '_'), axis=1)\n",
    "    training_data['body_fourgrams'] = training_data.apply(lambda row: getFourgram(row['body_unigrams'], '_'), axis=1)\n",
    "\n",
    "    training_data['head_unigrams'] = training_data.apply(lambda row: preprocess_data(row['head']), axis=1)\n",
    "    training_data['head_bigrams'] = training_data.apply(lambda row: getBigram(row['head_unigrams'], '_'), axis=1)\n",
    "    training_data['head_trigrams'] = training_data.apply(lambda row: getTrigram(row['head_unigrams'], '_'), axis=1)\n",
    "    training_data['head_fourgrams'] = training_data.apply(lambda row: getFourgram(row['head_unigrams'], '_'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cleaned text from unigrams\n",
    "if not USE_CACHE:\n",
    "    training_data['head_clean'] = training_data.apply(lambda row: ' '.join(row['head_unigrams']), axis=1)\n",
    "    training_data['body_clean'] = training_data.apply(lambda row: ' '.join(row['body_unigrams']), axis=1)\n",
    "    training_data['all_text'] = training_data.apply(lambda row: f\"{row['head_clean']} {row['body_clean']}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you sure you want to save? y\n",
      "saved\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to cache results\n",
    "if not USE_CACHE:\n",
    "    if input(\"Are you sure you want to save? \") == 'y':\n",
    "        with open(\"df.pickle\", 'wb') as handle:\n",
    "            handle.write(pickle.dumps(training_data))\n",
    "        print('saved')\n",
    "    else:\n",
    "        print('aborted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stance</th>\n",
       "      <th>head</th>\n",
       "      <th>body</th>\n",
       "      <th>body_tokens</th>\n",
       "      <th>head_tokens</th>\n",
       "      <th>body_unigrams</th>\n",
       "      <th>body_bigrams</th>\n",
       "      <th>body_trigrams</th>\n",
       "      <th>body_fourgrams</th>\n",
       "      <th>head_unigrams</th>\n",
       "      <th>head_bigrams</th>\n",
       "      <th>head_trigrams</th>\n",
       "      <th>head_fourgrams</th>\n",
       "      <th>head_clean</th>\n",
       "      <th>body_clean</th>\n",
       "      <th>all_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Hundreds of Palestinians flee floods in Gaza a...</td>\n",
       "      <td>Hundreds of Palestinians were evacuated from t...</td>\n",
       "      <td>[hundreds, of, palestinians, were, evacuated, ...</td>\n",
       "      <td>[hundreds, of, palestinians, flee, floods, in,...</td>\n",
       "      <td>[hundr, palestinian, evacu, home, sunday, morn...</td>\n",
       "      <td>[hundr_palestinian, palestinian_evacu, evacu_h...</td>\n",
       "      <td>[hundr_palestinian_evacu, palestinian_evacu_ho...</td>\n",
       "      <td>[hundr_palestinian_evacu_home, palestinian_eva...</td>\n",
       "      <td>[hundr, palestinian, flee, flood, gaza, israel...</td>\n",
       "      <td>[hundr_palestinian, palestinian_flee, flee_flo...</td>\n",
       "      <td>[hundr_palestinian_flee, palestinian_flee_floo...</td>\n",
       "      <td>[hundr_palestinian_flee_flood, palestinian_fle...</td>\n",
       "      <td>hundr palestinian flee flood gaza israel open dam</td>\n",
       "      <td>hundr palestinian evacu home sunday morn isra ...</td>\n",
       "      <td>hundr palestinian flee flood gaza israel open ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Spider burrowed through tourist's stomach and ...</td>\n",
       "      <td>Fear not arachnophobes, the story of Bunbury's...</td>\n",
       "      <td>[fear, not, arachnophobes, ,, the, story, of, ...</td>\n",
       "      <td>[spider, burrowed, through, tourist, 's, stoma...</td>\n",
       "      <td>[fear, arachnophob, stori, bunburi, spiderman,...</td>\n",
       "      <td>[fear_arachnophob, arachnophob_stori, stori_bu...</td>\n",
       "      <td>[fear_arachnophob_stori, arachnophob_stori_bun...</td>\n",
       "      <td>[fear_arachnophob_stori_bunburi, arachnophob_s...</td>\n",
       "      <td>[spider, burrow, tourist, stomach, chest]</td>\n",
       "      <td>[spider_burrow, burrow_tourist, tourist_stomac...</td>\n",
       "      <td>[spider_burrow_tourist, burrow_tourist_stomach...</td>\n",
       "      <td>[spider_burrow_tourist_stomach, burrow_tourist...</td>\n",
       "      <td>spider burrow tourist stomach chest</td>\n",
       "      <td>fear arachnophob stori bunburi spiderman might...</td>\n",
       "      <td>spider burrow tourist stomach chest fear arach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>'Nasa Confirms Earth Will Experience 6 Days of...</td>\n",
       "      <td>Thousands of people have been duped by a fake ...</td>\n",
       "      <td>[thousands, of, people, have, been, duped, by,...</td>\n",
       "      <td>['nasa, confirms, earth, will, experience, 6, ...</td>\n",
       "      <td>[thousand, peopl, dupe, fake, news, stori, cla...</td>\n",
       "      <td>[thousand_peopl, peopl_dupe, dupe_fake, fake_n...</td>\n",
       "      <td>[thousand_peopl_dupe, peopl_dupe_fake, dupe_fa...</td>\n",
       "      <td>[thousand_peopl_dupe_fake, peopl_dupe_fake_new...</td>\n",
       "      <td>[nasa, confirm, earth, experi, day, total, dar...</td>\n",
       "      <td>[nasa_confirm, confirm_earth, earth_experi, ex...</td>\n",
       "      <td>[nasa_confirm_earth, confirm_earth_experi, ear...</td>\n",
       "      <td>[nasa_confirm_earth_experi, confirm_earth_expe...</td>\n",
       "      <td>nasa confirm earth experi day total dark decem...</td>\n",
       "      <td>thousand peopl dupe fake news stori claim nasa...</td>\n",
       "      <td>nasa confirm earth experi day total dark decem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Banksy 'Arrested &amp; Real Identity Revealed' Is ...</td>\n",
       "      <td>If you’ve seen a story floating around on your...</td>\n",
       "      <td>[if, you, ’, ve, seen, a, story, floating, aro...</td>\n",
       "      <td>[banksy, 'arrested, &amp;, real, identity, reveale...</td>\n",
       "      <td>[seen, stori, float, around, facebook, feed, b...</td>\n",
       "      <td>[seen_stori, stori_float, float_around, around...</td>\n",
       "      <td>[seen_stori_float, stori_float_around, float_a...</td>\n",
       "      <td>[seen_stori_float_around, stori_float_around_f...</td>\n",
       "      <td>[banksi, arrest, real, ident, reveal, hoax, la...</td>\n",
       "      <td>[banksi_arrest, arrest_real, real_ident, ident...</td>\n",
       "      <td>[banksi_arrest_real, arrest_real_ident, real_i...</td>\n",
       "      <td>[banksi_arrest_real_ident, arrest_real_ident_r...</td>\n",
       "      <td>banksi arrest real ident reveal hoax last year</td>\n",
       "      <td>seen stori float around facebook feed banksi g...</td>\n",
       "      <td>banksi arrest real ident reveal hoax last year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Woman detained in Lebanon is not al-Baghdadi's...</td>\n",
       "      <td>An Iraqi official denied that a woman detained...</td>\n",
       "      <td>[an, iraqi, official, denied, that, a, woman, ...</td>\n",
       "      <td>[woman, detained, in, lebanon, is, not, al-bag...</td>\n",
       "      <td>[iraqi, offici, deni, woman, detain, lebanon, ...</td>\n",
       "      <td>[iraqi_offici, offici_deni, deni_woman, woman_...</td>\n",
       "      <td>[iraqi_offici_deni, offici_deni_woman, deni_wo...</td>\n",
       "      <td>[iraqi_offici_deni_woman, offici_deni_woman_de...</td>\n",
       "      <td>[woman, detain, lebanon, al, baghdadi, wife, i...</td>\n",
       "      <td>[woman_detain, detain_lebanon, lebanon_al, al_...</td>\n",
       "      <td>[woman_detain_lebanon, detain_lebanon_al, leba...</td>\n",
       "      <td>[woman_detain_lebanon_al, detain_lebanon_al_ba...</td>\n",
       "      <td>woman detain lebanon al baghdadi wife iraq say</td>\n",
       "      <td>iraqi offici deni woman detain lebanon wife ab...</td>\n",
       "      <td>woman detain lebanon al baghdadi wife iraq say...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Stance                                               head  \\\n",
       "0       1  Hundreds of Palestinians flee floods in Gaza a...   \n",
       "1       0  Spider burrowed through tourist's stomach and ...   \n",
       "2       1  'Nasa Confirms Earth Will Experience 6 Days of...   \n",
       "3       1  Banksy 'Arrested & Real Identity Revealed' Is ...   \n",
       "4       1  Woman detained in Lebanon is not al-Baghdadi's...   \n",
       "\n",
       "                                                body  \\\n",
       "0  Hundreds of Palestinians were evacuated from t...   \n",
       "1  Fear not arachnophobes, the story of Bunbury's...   \n",
       "2  Thousands of people have been duped by a fake ...   \n",
       "3  If you’ve seen a story floating around on your...   \n",
       "4  An Iraqi official denied that a woman detained...   \n",
       "\n",
       "                                         body_tokens  \\\n",
       "0  [hundreds, of, palestinians, were, evacuated, ...   \n",
       "1  [fear, not, arachnophobes, ,, the, story, of, ...   \n",
       "2  [thousands, of, people, have, been, duped, by,...   \n",
       "3  [if, you, ’, ve, seen, a, story, floating, aro...   \n",
       "4  [an, iraqi, official, denied, that, a, woman, ...   \n",
       "\n",
       "                                         head_tokens  \\\n",
       "0  [hundreds, of, palestinians, flee, floods, in,...   \n",
       "1  [spider, burrowed, through, tourist, 's, stoma...   \n",
       "2  ['nasa, confirms, earth, will, experience, 6, ...   \n",
       "3  [banksy, 'arrested, &, real, identity, reveale...   \n",
       "4  [woman, detained, in, lebanon, is, not, al-bag...   \n",
       "\n",
       "                                       body_unigrams  \\\n",
       "0  [hundr, palestinian, evacu, home, sunday, morn...   \n",
       "1  [fear, arachnophob, stori, bunburi, spiderman,...   \n",
       "2  [thousand, peopl, dupe, fake, news, stori, cla...   \n",
       "3  [seen, stori, float, around, facebook, feed, b...   \n",
       "4  [iraqi, offici, deni, woman, detain, lebanon, ...   \n",
       "\n",
       "                                        body_bigrams  \\\n",
       "0  [hundr_palestinian, palestinian_evacu, evacu_h...   \n",
       "1  [fear_arachnophob, arachnophob_stori, stori_bu...   \n",
       "2  [thousand_peopl, peopl_dupe, dupe_fake, fake_n...   \n",
       "3  [seen_stori, stori_float, float_around, around...   \n",
       "4  [iraqi_offici, offici_deni, deni_woman, woman_...   \n",
       "\n",
       "                                       body_trigrams  \\\n",
       "0  [hundr_palestinian_evacu, palestinian_evacu_ho...   \n",
       "1  [fear_arachnophob_stori, arachnophob_stori_bun...   \n",
       "2  [thousand_peopl_dupe, peopl_dupe_fake, dupe_fa...   \n",
       "3  [seen_stori_float, stori_float_around, float_a...   \n",
       "4  [iraqi_offici_deni, offici_deni_woman, deni_wo...   \n",
       "\n",
       "                                      body_fourgrams  \\\n",
       "0  [hundr_palestinian_evacu_home, palestinian_eva...   \n",
       "1  [fear_arachnophob_stori_bunburi, arachnophob_s...   \n",
       "2  [thousand_peopl_dupe_fake, peopl_dupe_fake_new...   \n",
       "3  [seen_stori_float_around, stori_float_around_f...   \n",
       "4  [iraqi_offici_deni_woman, offici_deni_woman_de...   \n",
       "\n",
       "                                       head_unigrams  \\\n",
       "0  [hundr, palestinian, flee, flood, gaza, israel...   \n",
       "1          [spider, burrow, tourist, stomach, chest]   \n",
       "2  [nasa, confirm, earth, experi, day, total, dar...   \n",
       "3  [banksi, arrest, real, ident, reveal, hoax, la...   \n",
       "4  [woman, detain, lebanon, al, baghdadi, wife, i...   \n",
       "\n",
       "                                        head_bigrams  \\\n",
       "0  [hundr_palestinian, palestinian_flee, flee_flo...   \n",
       "1  [spider_burrow, burrow_tourist, tourist_stomac...   \n",
       "2  [nasa_confirm, confirm_earth, earth_experi, ex...   \n",
       "3  [banksi_arrest, arrest_real, real_ident, ident...   \n",
       "4  [woman_detain, detain_lebanon, lebanon_al, al_...   \n",
       "\n",
       "                                       head_trigrams  \\\n",
       "0  [hundr_palestinian_flee, palestinian_flee_floo...   \n",
       "1  [spider_burrow_tourist, burrow_tourist_stomach...   \n",
       "2  [nasa_confirm_earth, confirm_earth_experi, ear...   \n",
       "3  [banksi_arrest_real, arrest_real_ident, real_i...   \n",
       "4  [woman_detain_lebanon, detain_lebanon_al, leba...   \n",
       "\n",
       "                                      head_fourgrams  \\\n",
       "0  [hundr_palestinian_flee_flood, palestinian_fle...   \n",
       "1  [spider_burrow_tourist_stomach, burrow_tourist...   \n",
       "2  [nasa_confirm_earth_experi, confirm_earth_expe...   \n",
       "3  [banksi_arrest_real_ident, arrest_real_ident_r...   \n",
       "4  [woman_detain_lebanon_al, detain_lebanon_al_ba...   \n",
       "\n",
       "                                          head_clean  \\\n",
       "0  hundr palestinian flee flood gaza israel open dam   \n",
       "1                spider burrow tourist stomach chest   \n",
       "2  nasa confirm earth experi day total dark decem...   \n",
       "3     banksi arrest real ident reveal hoax last year   \n",
       "4     woman detain lebanon al baghdadi wife iraq say   \n",
       "\n",
       "                                          body_clean  \\\n",
       "0  hundr palestinian evacu home sunday morn isra ...   \n",
       "1  fear arachnophob stori bunburi spiderman might...   \n",
       "2  thousand peopl dupe fake news stori claim nasa...   \n",
       "3  seen stori float around facebook feed banksi g...   \n",
       "4  iraqi offici deni woman detain lebanon wife ab...   \n",
       "\n",
       "                                            all_text  \n",
       "0  hundr palestinian flee flood gaza israel open ...  \n",
       "1  spider burrow tourist stomach chest fear arach...  \n",
       "2  nasa confirm earth experi day total dark decem...  \n",
       "3  banksi arrest real ident reveal hoax last year...  \n",
       "4  woman detain lebanon al baghdadi wife iraq say...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "with io.open(\"training_data.csv\", \"w\") as handle:\n",
    "    handle.write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Simple ML Magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate features + respective training sets\n",
    "# simple test with 75% of the dataset used for training, 25% used for testing\n",
    "partition = int(0.75 * len(training_data))\n",
    "\n",
    "features = [CountFeatureGenerator(), TfidfFeatureGenerator()]\n",
    "\n",
    "for feature in features:\n",
    "    feature.process(training_data.copy())\n",
    "    \n",
    "tree_training_datasets = []\n",
    "for feature in features:\n",
    "    for training_dataset in feature.read():\n",
    "        x_train = training_dataset[:partition]\n",
    "        x_test = training_dataset[partition:]\n",
    "        tree_training_datasets.append((feature._name, x_train, x_test))\n",
    "\n",
    "y_train = training_data['Stance'].values[:partition]\n",
    "y_test = training_data['Stance'].values[partition:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score on countFeatureGenerator using a GradientBoostingClassifier: 0.9089968976215098 (accuracy: 0.8442477876106195)\n",
      "f1 score on tfidfFeatureGenerator using a GradientBoostingClassifier: 0.8947087119187601 (accuracy: 0.8256637168141593)\n",
      "f1 score on tfidfFeatureGenerator using a GradientBoostingClassifier: 0.8952991452991453 (accuracy: 0.8265486725663717)\n",
      "f1 score on tfidfFeatureGenerator using a GradientBoostingClassifier: 0.8789873417721519 (accuracy: 0.7884955752212389)\n",
      "f1 score on countFeatureGenerator using an SVC: 0.9045079980610762 (accuracy: 0.8256637168141593)\n",
      "f1 score on tfidfFeatureGenerator using an SVC: 0.9030883919062832 (accuracy: 0.8389380530973451)\n",
      "f1 score on tfidfFeatureGenerator using an SVC: 0.9030883919062832 (accuracy: 0.8389380530973451)\n",
      "f1 score on tfidfFeatureGenerator using an SVC: 0.9045079980610762 (accuracy: 0.8256637168141593)\n"
     ]
    }
   ],
   "source": [
    "# Run a GradientBoostingClassifier/SVC on each feature and observe their f1 scores\n",
    "for (feature, x_train, x_test) in tree_training_datasets:\n",
    "    clf = GradientBoostingClassifier(n_estimators=300, max_depth=5).fit(x_train, y_train)\n",
    "    guess = clf.predict(x_test)\n",
    "    score = f1_score(y_test, guess)\n",
    "    print(f\"f1 score on {feature} using a GradientBoostingClassifier: {score} (accuracy: {(guess == y_test).mean()})\")\n",
    "\n",
    "for (feature, x_train, x_test) in tree_training_datasets:\n",
    "    clf = SVC().fit(x_train, y_train)\n",
    "    guess = clf.predict(x_test)\n",
    "    score = f1_score(y_test, guess)\n",
    "    print(f\"f1 score on {feature} using an SVC: {score} (accuracy: {(guess == y_test).mean()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8256637168141593"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all tables into one giant table, and run a few ensemble methods on it\n",
    "master_df = pd.DataFrame()\n",
    "for (_, x_train, x_test) in tree_training_datasets:\n",
    "    master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
