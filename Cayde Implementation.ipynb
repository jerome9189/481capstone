{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 481n Project Advanced Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cayde\n",
    "from cayde.well.nlpwell import NLPWell\n",
    "from cayde.classifier.dtree import DecisionTreeClassifier\n",
    "from cayde.classifier.ada import AdaBoosterClassifier\n",
    "from cayde.classifier.svm import SVMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "well = NLPWell(\"training_data.csv\")\n",
    "well.fetch()\n",
    "\n",
    "well.input_cols = well.text_cols = [\"head\", \"body\"]\n",
    "well.output_col = \"Stance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hundreds of Palestinians flee floods in Gaza a...</td>\n",
       "      <td>Hundreds of Palestinians were evacuated from t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spider burrowed through tourist's stomach and ...</td>\n",
       "      <td>Fear not arachnophobes, the story of Bunbury's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'Nasa Confirms Earth Will Experience 6 Days of...</td>\n",
       "      <td>Thousands of people have been duped by a fake ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Banksy 'Arrested &amp; Real Identity Revealed' Is ...</td>\n",
       "      <td>If you’ve seen a story floating around on your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Woman detained in Lebanon is not al-Baghdadi's...</td>\n",
       "      <td>An Iraqi official denied that a woman detained...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4513</th>\n",
       "      <td>Tropical spider burrows under man's skin throu...</td>\n",
       "      <td>A trip to Bali has turned Dylan Thomas into a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4514</th>\n",
       "      <td>Tour, agent: Tiger Woods not banned</td>\n",
       "      <td>PALM BEACH GARDENS, Fla. -- A journeyman profe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4515</th>\n",
       "      <td>Meteorite leaves crater in Nicaraguan capital ...</td>\n",
       "      <td>A blast near the Nicaraguan capital city of Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4516</th>\n",
       "      <td>ESPN to save NFL's image with all-male domesti...</td>\n",
       "      <td>Tonight — finally! — ESPN is going to have an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4517</th>\n",
       "      <td>Mexico Says Missing Students Not Found In Firs...</td>\n",
       "      <td>The bodies found in a mass grave were confirme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4518 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   head  \\\n",
       "0     Hundreds of Palestinians flee floods in Gaza a...   \n",
       "1     Spider burrowed through tourist's stomach and ...   \n",
       "2     'Nasa Confirms Earth Will Experience 6 Days of...   \n",
       "3     Banksy 'Arrested & Real Identity Revealed' Is ...   \n",
       "4     Woman detained in Lebanon is not al-Baghdadi's...   \n",
       "...                                                 ...   \n",
       "4513  Tropical spider burrows under man's skin throu...   \n",
       "4514                Tour, agent: Tiger Woods not banned   \n",
       "4515  Meteorite leaves crater in Nicaraguan capital ...   \n",
       "4516  ESPN to save NFL's image with all-male domesti...   \n",
       "4517  Mexico Says Missing Students Not Found In Firs...   \n",
       "\n",
       "                                                   body  \n",
       "0     Hundreds of Palestinians were evacuated from t...  \n",
       "1     Fear not arachnophobes, the story of Bunbury's...  \n",
       "2     Thousands of people have been duped by a fake ...  \n",
       "3     If you’ve seen a story floating around on your...  \n",
       "4     An Iraqi official denied that a woman detained...  \n",
       "...                                                 ...  \n",
       "4513  A trip to Bali has turned Dylan Thomas into a ...  \n",
       "4514  PALM BEACH GARDENS, Fla. -- A journeyman profe...  \n",
       "4515  A blast near the Nicaraguan capital city of Ma...  \n",
       "4516  Tonight — finally! — ESPN is going to have an ...  \n",
       "4517  The bodies found in a mass grave were confirme...  \n",
       "\n",
       "[4518 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "well.input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work with a smaller dataset in a non-google TPU environment\n",
    "well = well.getToySample(maxRows=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well.clean_text(package='bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Stance</th>\n",
       "      <th>head</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>1489</td>\n",
       "      <td>1</td>\n",
       "      <td>ESPN's Domestic Violence Panel Is Missing Some...</td>\n",
       "      <td>Actual women.\\r\\n\\r\\nWith ongoing outrage over...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>211</td>\n",
       "      <td>1</td>\n",
       "      <td>Sadly, Pumpkin Spice Condoms Aren't A Thing Af...</td>\n",
       "      <td>Forget sweater weather and crisp autumn leaves...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  Stance                                               head  \\\n",
       "1489   1489       1  ESPN's Domestic Violence Panel Is Missing Some...   \n",
       "211     211       1  Sadly, Pumpkin Spice Condoms Aren't A Thing Af...   \n",
       "\n",
       "                                                   body  \n",
       "1489  Actual women.\\r\\n\\r\\nWith ongoing outrage over...  \n",
       "211   Forget sweater weather and crisp autumn leaves...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "well._df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well._df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] seven high school girls get pregnant on five - day school trip , parents outraged [SEP] on december 17 , a site called ins ##er ##bia . info published a 400 - word piece under the sensation ##al headline : bi ##h : seven primary school students pregnant after five - day excursion “ bi ##h ” presumably refers to — according to the article — the ministry run by serbia ’ s national coordinator for reproductive health , ne ##nad ba ##bic ##i . ba ##bic ##i is the only source the article uses for the story . the original article has now been linked by the new york post , the london daily mail , the new york daily news , australia ’ s [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] seven high school girls get pregnant on five - day school trip , parents outraged [SEP] on december 17 , a site called ins ##er ##bia . info published a 400 - word piece under the sensation ##al headline : bi ##h : seven primary school students pregnant after five - day excursion “ bi ##h ” presumably refers to — according to the article — the ministry run by serbia ’ s national coordinator for reproductive health , ne ##nad ba ##bic ##i . ba ##bic ##i is the only source the article uses for the story . the original article has now been linked by the new york post , the london daily mail , the new york daily news , australia ’ s [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2698 2152 2082 3057 2131 6875 2006 2274 1011 2154 2082 4440 1010 3008 23558 102 2006 2285 2459 1010 1037 2609 2170 16021 2121 11607 1012 18558 2405 1037 4278 1011 2773 3538 2104 1996 8742 2389 17653 1024 12170 2232 1024 2698 3078 2082 2493 6875 2044 2274 1011 2154 26144 1523 12170 2232 1524 10712 5218 2000 1517 2429 2000 1996 3720 1517 1996 3757 2448 2011 7238 1521 1055 2120 10669 2005 15124 2740 1010 11265 25389 8670 13592 2072 1012 8670 13592 2072 2003 1996 2069 3120 1996 3720 3594 2005 1996 2466 1012 1996 2434 3720 2038 2085 2042 5799 2011 1996 2047 2259 2695 1010 1996 2414 3679 5653 1010 1996 2047 2259 3679 2739 1010 2660 1521 1055 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2698 2152 2082 3057 2131 6875 2006 2274 1011 2154 2082 4440 1010 3008 23558 102 2006 2285 2459 1010 1037 2609 2170 16021 2121 11607 1012 18558 2405 1037 4278 1011 2773 3538 2104 1996 8742 2389 17653 1024 12170 2232 1024 2698 3078 2082 2493 6875 2044 2274 1011 2154 26144 1523 12170 2232 1524 10712 5218 2000 1517 2429 2000 1996 3720 1517 1996 3757 2448 2011 7238 1521 1055 2120 10669 2005 15124 2740 1010 11265 25389 8670 13592 2072 1012 8670 13592 2072 2003 1996 2069 3120 1996 3720 3594 2005 1996 2466 1012 1996 2434 3720 2038 2085 2042 5799 2011 1996 2047 2259 2695 1010 1996 2414 3679 5653 1010 1996 2047 2259 3679 2739 1010 2660 1521 1055 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] an indian civil servant just got sacked after not turning up for work for 25 years [SEP] ( reuters ) - even in india , where government jobs are considered to be for life , a . k . ve ##rma was pushing it . ve ##rma , an executive engineer at the central public works department , was fired after last appearing for work in december 1990 . \" he went on seeking extension of leave , which was not sanctioned , and def ##ied directions to report to work , \" the government said in a statement on thursday . even after an inquiry found him guilty of \" wil ##ful absence from duty \" in 1992 , it took another 22 years and [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] an indian civil servant just got sacked after not turning up for work for 25 years [SEP] ( reuters ) - even in india , where government jobs are considered to be for life , a . k . ve ##rma was pushing it . ve ##rma , an executive engineer at the central public works department , was fired after last appearing for work in december 1990 . \" he went on seeking extension of leave , which was not sanctioned , and def ##ied directions to report to work , \" the government said in a statement on thursday . even after an inquiry found him guilty of \" wil ##ful absence from duty \" in 1992 , it took another 22 years and [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2019 2796 2942 7947 2074 2288 14159 2044 2025 3810 2039 2005 2147 2005 2423 2086 102 1006 26665 1007 1011 2130 1999 2634 1010 2073 2231 5841 2024 2641 2000 2022 2005 2166 1010 1037 1012 1047 1012 2310 17830 2001 6183 2009 1012 2310 17830 1010 2019 3237 3992 2012 1996 2430 2270 2573 2533 1010 2001 5045 2044 2197 6037 2005 2147 1999 2285 2901 1012 1000 2002 2253 2006 6224 5331 1997 2681 1010 2029 2001 2025 14755 1010 1998 13366 6340 7826 2000 3189 2000 2147 1010 1000 1996 2231 2056 1999 1037 4861 2006 9432 1012 2130 2044 2019 9934 2179 2032 5905 1997 1000 19863 3993 6438 2013 4611 1000 1999 2826 1010 2009 2165 2178 2570 2086 1998 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2019 2796 2942 7947 2074 2288 14159 2044 2025 3810 2039 2005 2147 2005 2423 2086 102 1006 26665 1007 1011 2130 1999 2634 1010 2073 2231 5841 2024 2641 2000 2022 2005 2166 1010 1037 1012 1047 1012 2310 17830 2001 6183 2009 1012 2310 17830 1010 2019 3237 3992 2012 1996 2430 2270 2573 2533 1010 2001 5045 2044 2197 6037 2005 2147 1999 2285 2901 1012 1000 2002 2253 2006 6224 5331 1997 2681 1010 2029 2001 2025 14755 1010 1998 13366 6340 7826 2000 3189 2000 2147 1010 1000 1996 2231 2056 1999 1037 4861 2006 9432 1012 2130 2044 2019 9934 2179 2032 5905 1997 1000 19863 3993 6438 2013 4611 1000 1999 2826 1010 2009 2165 2178 2570 2086 1998 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['head_body_bert']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "well.createBERTEncodings(compareTextMode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3598    <bert.run_classifier.InputFeatures object at 0...\n",
       "532     <bert.run_classifier.InputFeatures object at 0...\n",
       "Name: head_body_bert, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "well._df['head_body_bert']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "def create_tokenizer_from_hub_module(bert_model_hub: str) -> bert.tokenization.FullTokenizer:\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(bert_model_hub)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        with tf.Session() as sess:\n",
    "              vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"], tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "    return bert.tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module(\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list of new columns available for usage\n",
    "# This creates columns where the text is stemmed and cleaned\n",
    "well.clean_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well.input_cols = ['head', 'body', 'head_clean', 'body_1gram']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates columns where the text is converted to bigrams/trigrams\n",
    "well.createNgrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the word exist in each text_column?\n",
    "well.createCountWordFeatures([\n",
    "    'fake',\n",
    "    'fraud',\n",
    "    'hoax',\n",
    "    'false',\n",
    "    'deny', 'denies',\n",
    "    'refute',\n",
    "    'not',\n",
    "    'despite',\n",
    "    'nope',\n",
    "    'doubt', 'doubts',\n",
    "    'bogus',\n",
    "    'debunk',\n",
    "    'pranks',\n",
    "    'retract'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well.input_cols = production_cols = [\n",
    " 'head_2gram_unique_ratio',\n",
    " 'head_3gram_unique_ratio',\n",
    " 'head_4gram_unique_ratio',\n",
    " 'body_2gram_unique_ratio',\n",
    " 'body_3gram_unique_ratio',\n",
    " 'body_4gram_unique_ratio'\n",
    "] + ['head_contains_fake',\n",
    " 'head_contains_fraud',\n",
    " 'head_contains_hoax',\n",
    " 'head_contains_false',\n",
    " 'head_contains_deny',\n",
    " 'head_contains_denies',\n",
    " 'head_contains_refute',\n",
    " 'head_contains_not',\n",
    " 'head_contains_despite',\n",
    " 'head_contains_nope',\n",
    " 'head_contains_doubt',\n",
    " 'head_contains_doubts',\n",
    " 'head_contains_bogus',\n",
    " 'head_contains_debunk',\n",
    " 'head_contains_pranks',\n",
    " 'head_contains_retract',\n",
    " 'body_contains_fake',\n",
    " 'body_contains_fraud',\n",
    " 'body_contains_hoax',\n",
    " 'body_contains_false',\n",
    " 'body_contains_deny',\n",
    " 'body_contains_denies',\n",
    " 'body_contains_refute',\n",
    " 'body_contains_not',\n",
    " 'body_contains_despite',\n",
    " 'body_contains_nope',\n",
    " 'body_contains_doubt',\n",
    " 'body_contains_doubts',\n",
    " 'body_contains_bogus',\n",
    " 'body_contains_debunk',\n",
    " 'body_contains_pranks',\n",
    " 'body_contains_retract'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(well)\n",
    "clf.fit()\n",
    "clf.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoosterClassifier(well, n_estimators=150)\n",
    "clf.fit()\n",
    "clf.accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVMClassifier(well)\n",
    "clf.fit()\n",
    "clf.accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrating BERT & Cayde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "well.createBERTEncodings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well._df['head_bert']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(well._df['head_bert'][1758])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
