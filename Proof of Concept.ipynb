{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof of Concept Strawman\n",
    "\n",
    "Here we demonstrate how using simple methods like multiple gradient boosted trees can help achieve our goals. Inspiration taken from Talos and the UCLNLP group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import word_tokenize\n",
    "from helpers import preprocess_data\n",
    "from ngram import getBigram, getTrigram, getFourgram\n",
    "from featureExtractors import CountFeatureGenerator, TfidfFeatureGenerator\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score, precision_recall_curve, plot_precision_recall_curve\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CACHE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size: 4518 entries\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to read from the cache\n",
    "if USE_CACHE:\n",
    "    with open(\"df.pickle\", \"rb\") as handle:\n",
    "        training_data = pickle.loads(handle.read())\n",
    "    print(f\"Training Data Size: {len(training_data)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading entire dataset into a single dataframe\n",
    "if not USE_CACHE:\n",
    "    bodies = pd.read_csv(\"train_bodies.csv\").set_index('Body ID')\n",
    "    training_data = pd.read_csv(\"train_stances.csv\")\n",
    "    training_data['head'] = training_data['Headline']\n",
    "    training_data['body'] = bodies['articleBody'][training_data['Body ID']].reset_index(drop=True)\n",
    "    del training_data['Body ID']\n",
    "    del training_data['Headline']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out all \"unrelated/discuss items\"\n",
    "if not USE_CACHE:\n",
    "    training_data = training_data[training_data['Stance'] != 'unrelated']\n",
    "    training_data = training_data[training_data['Stance'] != 'discuss']\n",
    "    training_data['Stance'] = training_data['Stance'] == 'agree'\n",
    "    training_data['Stance'] = training_data['Stance'].astype(int)\n",
    "    training_data = training_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tokenize the respective contents\n",
    "if not USE_CACHE:\n",
    "    training_data['body_tokens'] = training_data.apply(lambda row: word_tokenize(row['body'].lower()), axis=1)\n",
    "    training_data['head_tokens'] = training_data.apply(lambda row: word_tokenize(row['head'].lower()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create n-grams\n",
    "if not USE_CACHE:\n",
    "    training_data['body_unigrams'] = training_data.apply(lambda row: preprocess_data(row['body']), axis=1)\n",
    "    training_data['body_bigrams'] = training_data.apply(lambda row: getBigram(row['body_unigrams'], '_'), axis=1)\n",
    "    training_data['body_trigrams'] = training_data.apply(lambda row: getTrigram(row['body_unigrams'], '_'), axis=1)\n",
    "    training_data['body_fourgrams'] = training_data.apply(lambda row: getFourgram(row['body_unigrams'], '_'), axis=1)\n",
    "\n",
    "    training_data['head_unigrams'] = training_data.apply(lambda row: preprocess_data(row['head']), axis=1)\n",
    "    training_data['head_bigrams'] = training_data.apply(lambda row: getBigram(row['head_unigrams'], '_'), axis=1)\n",
    "    training_data['head_trigrams'] = training_data.apply(lambda row: getTrigram(row['head_unigrams'], '_'), axis=1)\n",
    "    training_data['head_fourgrams'] = training_data.apply(lambda row: getFourgram(row['head_unigrams'], '_'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cleaned text from unigrams\n",
    "if not USE_CACHE:\n",
    "    training_data['head_clean'] = training_data.apply(lambda row: ' '.join(row['head_unigrams']), axis=1)\n",
    "    training_data['body_clean'] = training_data.apply(lambda row: ' '.join(row['body_unigrams']), axis=1)\n",
    "    training_data['all_text'] = training_data.apply(lambda row: f\"{row['head_clean']} {row['body_clean']}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to cache results\n",
    "if not USE_CACHE:\n",
    "    if input(\"Are you sure you want to save? \") == 'y':\n",
    "        with open(\"df.pickle\", 'wb') as handle:\n",
    "            handle.write(pickle.dumps(training_data))\n",
    "        print('saved')\n",
    "    else:\n",
    "        print('aborted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stance</th>\n",
       "      <th>head</th>\n",
       "      <th>body</th>\n",
       "      <th>body_tokens</th>\n",
       "      <th>head_tokens</th>\n",
       "      <th>body_unigrams</th>\n",
       "      <th>body_bigrams</th>\n",
       "      <th>body_trigrams</th>\n",
       "      <th>body_fourgrams</th>\n",
       "      <th>head_unigrams</th>\n",
       "      <th>head_bigrams</th>\n",
       "      <th>head_trigrams</th>\n",
       "      <th>head_fourgrams</th>\n",
       "      <th>head_clean</th>\n",
       "      <th>body_clean</th>\n",
       "      <th>all_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Hundreds of Palestinians flee floods in Gaza a...</td>\n",
       "      <td>Hundreds of Palestinians were evacuated from t...</td>\n",
       "      <td>[hundreds, of, palestinians, were, evacuated, ...</td>\n",
       "      <td>[hundreds, of, palestinians, flee, floods, in,...</td>\n",
       "      <td>[hundr, palestinian, evacu, home, sunday, morn...</td>\n",
       "      <td>[hundr_palestinian, palestinian_evacu, evacu_h...</td>\n",
       "      <td>[hundr_palestinian_evacu, palestinian_evacu_ho...</td>\n",
       "      <td>[hundr_palestinian_evacu_home, palestinian_eva...</td>\n",
       "      <td>[hundr, palestinian, flee, flood, gaza, israel...</td>\n",
       "      <td>[hundr_palestinian, palestinian_flee, flee_flo...</td>\n",
       "      <td>[hundr_palestinian_flee, palestinian_flee_floo...</td>\n",
       "      <td>[hundr_palestinian_flee_flood, palestinian_fle...</td>\n",
       "      <td>hundr palestinian flee flood gaza israel open dam</td>\n",
       "      <td>hundr palestinian evacu home sunday morn isra ...</td>\n",
       "      <td>hundr palestinian flee flood gaza israel open ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Spider burrowed through tourist's stomach and ...</td>\n",
       "      <td>Fear not arachnophobes, the story of Bunbury's...</td>\n",
       "      <td>[fear, not, arachnophobes, ,, the, story, of, ...</td>\n",
       "      <td>[spider, burrowed, through, tourist, 's, stoma...</td>\n",
       "      <td>[fear, arachnophob, stori, bunburi, spiderman,...</td>\n",
       "      <td>[fear_arachnophob, arachnophob_stori, stori_bu...</td>\n",
       "      <td>[fear_arachnophob_stori, arachnophob_stori_bun...</td>\n",
       "      <td>[fear_arachnophob_stori_bunburi, arachnophob_s...</td>\n",
       "      <td>[spider, burrow, tourist, stomach, chest]</td>\n",
       "      <td>[spider_burrow, burrow_tourist, tourist_stomac...</td>\n",
       "      <td>[spider_burrow_tourist, burrow_tourist_stomach...</td>\n",
       "      <td>[spider_burrow_tourist_stomach, burrow_tourist...</td>\n",
       "      <td>spider burrow tourist stomach chest</td>\n",
       "      <td>fear arachnophob stori bunburi spiderman might...</td>\n",
       "      <td>spider burrow tourist stomach chest fear arach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>'Nasa Confirms Earth Will Experience 6 Days of...</td>\n",
       "      <td>Thousands of people have been duped by a fake ...</td>\n",
       "      <td>[thousands, of, people, have, been, duped, by,...</td>\n",
       "      <td>['nasa, confirms, earth, will, experience, 6, ...</td>\n",
       "      <td>[thousand, peopl, dupe, fake, news, stori, cla...</td>\n",
       "      <td>[thousand_peopl, peopl_dupe, dupe_fake, fake_n...</td>\n",
       "      <td>[thousand_peopl_dupe, peopl_dupe_fake, dupe_fa...</td>\n",
       "      <td>[thousand_peopl_dupe_fake, peopl_dupe_fake_new...</td>\n",
       "      <td>[nasa, confirm, earth, experi, day, total, dar...</td>\n",
       "      <td>[nasa_confirm, confirm_earth, earth_experi, ex...</td>\n",
       "      <td>[nasa_confirm_earth, confirm_earth_experi, ear...</td>\n",
       "      <td>[nasa_confirm_earth_experi, confirm_earth_expe...</td>\n",
       "      <td>nasa confirm earth experi day total dark decem...</td>\n",
       "      <td>thousand peopl dupe fake news stori claim nasa...</td>\n",
       "      <td>nasa confirm earth experi day total dark decem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Banksy 'Arrested &amp; Real Identity Revealed' Is ...</td>\n",
       "      <td>If you’ve seen a story floating around on your...</td>\n",
       "      <td>[if, you, ’, ve, seen, a, story, floating, aro...</td>\n",
       "      <td>[banksy, 'arrested, &amp;, real, identity, reveale...</td>\n",
       "      <td>[seen, stori, float, around, facebook, feed, b...</td>\n",
       "      <td>[seen_stori, stori_float, float_around, around...</td>\n",
       "      <td>[seen_stori_float, stori_float_around, float_a...</td>\n",
       "      <td>[seen_stori_float_around, stori_float_around_f...</td>\n",
       "      <td>[banksi, arrest, real, ident, reveal, hoax, la...</td>\n",
       "      <td>[banksi_arrest, arrest_real, real_ident, ident...</td>\n",
       "      <td>[banksi_arrest_real, arrest_real_ident, real_i...</td>\n",
       "      <td>[banksi_arrest_real_ident, arrest_real_ident_r...</td>\n",
       "      <td>banksi arrest real ident reveal hoax last year</td>\n",
       "      <td>seen stori float around facebook feed banksi g...</td>\n",
       "      <td>banksi arrest real ident reveal hoax last year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Woman detained in Lebanon is not al-Baghdadi's...</td>\n",
       "      <td>An Iraqi official denied that a woman detained...</td>\n",
       "      <td>[an, iraqi, official, denied, that, a, woman, ...</td>\n",
       "      <td>[woman, detained, in, lebanon, is, not, al-bag...</td>\n",
       "      <td>[iraqi, offici, deni, woman, detain, lebanon, ...</td>\n",
       "      <td>[iraqi_offici, offici_deni, deni_woman, woman_...</td>\n",
       "      <td>[iraqi_offici_deni, offici_deni_woman, deni_wo...</td>\n",
       "      <td>[iraqi_offici_deni_woman, offici_deni_woman_de...</td>\n",
       "      <td>[woman, detain, lebanon, al, baghdadi, wife, i...</td>\n",
       "      <td>[woman_detain, detain_lebanon, lebanon_al, al_...</td>\n",
       "      <td>[woman_detain_lebanon, detain_lebanon_al, leba...</td>\n",
       "      <td>[woman_detain_lebanon_al, detain_lebanon_al_ba...</td>\n",
       "      <td>woman detain lebanon al baghdadi wife iraq say</td>\n",
       "      <td>iraqi offici deni woman detain lebanon wife ab...</td>\n",
       "      <td>woman detain lebanon al baghdadi wife iraq say...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Stance                                               head  \\\n",
       "0       1  Hundreds of Palestinians flee floods in Gaza a...   \n",
       "1       0  Spider burrowed through tourist's stomach and ...   \n",
       "2       1  'Nasa Confirms Earth Will Experience 6 Days of...   \n",
       "3       1  Banksy 'Arrested & Real Identity Revealed' Is ...   \n",
       "4       1  Woman detained in Lebanon is not al-Baghdadi's...   \n",
       "\n",
       "                                                body  \\\n",
       "0  Hundreds of Palestinians were evacuated from t...   \n",
       "1  Fear not arachnophobes, the story of Bunbury's...   \n",
       "2  Thousands of people have been duped by a fake ...   \n",
       "3  If you’ve seen a story floating around on your...   \n",
       "4  An Iraqi official denied that a woman detained...   \n",
       "\n",
       "                                         body_tokens  \\\n",
       "0  [hundreds, of, palestinians, were, evacuated, ...   \n",
       "1  [fear, not, arachnophobes, ,, the, story, of, ...   \n",
       "2  [thousands, of, people, have, been, duped, by,...   \n",
       "3  [if, you, ’, ve, seen, a, story, floating, aro...   \n",
       "4  [an, iraqi, official, denied, that, a, woman, ...   \n",
       "\n",
       "                                         head_tokens  \\\n",
       "0  [hundreds, of, palestinians, flee, floods, in,...   \n",
       "1  [spider, burrowed, through, tourist, 's, stoma...   \n",
       "2  ['nasa, confirms, earth, will, experience, 6, ...   \n",
       "3  [banksy, 'arrested, &, real, identity, reveale...   \n",
       "4  [woman, detained, in, lebanon, is, not, al-bag...   \n",
       "\n",
       "                                       body_unigrams  \\\n",
       "0  [hundr, palestinian, evacu, home, sunday, morn...   \n",
       "1  [fear, arachnophob, stori, bunburi, spiderman,...   \n",
       "2  [thousand, peopl, dupe, fake, news, stori, cla...   \n",
       "3  [seen, stori, float, around, facebook, feed, b...   \n",
       "4  [iraqi, offici, deni, woman, detain, lebanon, ...   \n",
       "\n",
       "                                        body_bigrams  \\\n",
       "0  [hundr_palestinian, palestinian_evacu, evacu_h...   \n",
       "1  [fear_arachnophob, arachnophob_stori, stori_bu...   \n",
       "2  [thousand_peopl, peopl_dupe, dupe_fake, fake_n...   \n",
       "3  [seen_stori, stori_float, float_around, around...   \n",
       "4  [iraqi_offici, offici_deni, deni_woman, woman_...   \n",
       "\n",
       "                                       body_trigrams  \\\n",
       "0  [hundr_palestinian_evacu, palestinian_evacu_ho...   \n",
       "1  [fear_arachnophob_stori, arachnophob_stori_bun...   \n",
       "2  [thousand_peopl_dupe, peopl_dupe_fake, dupe_fa...   \n",
       "3  [seen_stori_float, stori_float_around, float_a...   \n",
       "4  [iraqi_offici_deni, offici_deni_woman, deni_wo...   \n",
       "\n",
       "                                      body_fourgrams  \\\n",
       "0  [hundr_palestinian_evacu_home, palestinian_eva...   \n",
       "1  [fear_arachnophob_stori_bunburi, arachnophob_s...   \n",
       "2  [thousand_peopl_dupe_fake, peopl_dupe_fake_new...   \n",
       "3  [seen_stori_float_around, stori_float_around_f...   \n",
       "4  [iraqi_offici_deni_woman, offici_deni_woman_de...   \n",
       "\n",
       "                                       head_unigrams  \\\n",
       "0  [hundr, palestinian, flee, flood, gaza, israel...   \n",
       "1          [spider, burrow, tourist, stomach, chest]   \n",
       "2  [nasa, confirm, earth, experi, day, total, dar...   \n",
       "3  [banksi, arrest, real, ident, reveal, hoax, la...   \n",
       "4  [woman, detain, lebanon, al, baghdadi, wife, i...   \n",
       "\n",
       "                                        head_bigrams  \\\n",
       "0  [hundr_palestinian, palestinian_flee, flee_flo...   \n",
       "1  [spider_burrow, burrow_tourist, tourist_stomac...   \n",
       "2  [nasa_confirm, confirm_earth, earth_experi, ex...   \n",
       "3  [banksi_arrest, arrest_real, real_ident, ident...   \n",
       "4  [woman_detain, detain_lebanon, lebanon_al, al_...   \n",
       "\n",
       "                                       head_trigrams  \\\n",
       "0  [hundr_palestinian_flee, palestinian_flee_floo...   \n",
       "1  [spider_burrow_tourist, burrow_tourist_stomach...   \n",
       "2  [nasa_confirm_earth, confirm_earth_experi, ear...   \n",
       "3  [banksi_arrest_real, arrest_real_ident, real_i...   \n",
       "4  [woman_detain_lebanon, detain_lebanon_al, leba...   \n",
       "\n",
       "                                      head_fourgrams  \\\n",
       "0  [hundr_palestinian_flee_flood, palestinian_fle...   \n",
       "1  [spider_burrow_tourist_stomach, burrow_tourist...   \n",
       "2  [nasa_confirm_earth_experi, confirm_earth_expe...   \n",
       "3  [banksi_arrest_real_ident, arrest_real_ident_r...   \n",
       "4  [woman_detain_lebanon_al, detain_lebanon_al_ba...   \n",
       "\n",
       "                                          head_clean  \\\n",
       "0  hundr palestinian flee flood gaza israel open dam   \n",
       "1                spider burrow tourist stomach chest   \n",
       "2  nasa confirm earth experi day total dark decem...   \n",
       "3     banksi arrest real ident reveal hoax last year   \n",
       "4     woman detain lebanon al baghdadi wife iraq say   \n",
       "\n",
       "                                          body_clean  \\\n",
       "0  hundr palestinian evacu home sunday morn isra ...   \n",
       "1  fear arachnophob stori bunburi spiderman might...   \n",
       "2  thousand peopl dupe fake news stori claim nasa...   \n",
       "3  seen stori float around facebook feed banksi g...   \n",
       "4  iraqi offici deni woman detain lebanon wife ab...   \n",
       "\n",
       "                                            all_text  \n",
       "0  hundr palestinian flee flood gaza israel open ...  \n",
       "1  spider burrow tourist stomach chest fear arach...  \n",
       "2  nasa confirm earth experi day total dark decem...  \n",
       "3  banksi arrest real ident reveal hoax last year...  \n",
       "4  woman detain lebanon al baghdadi wife iraq say...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratchwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_of_head_unigrams</th>\n",
       "      <th>count_of_unique_head_unigrams</th>\n",
       "      <th>ratio_of_unique_head_unigrams</th>\n",
       "      <th>count_of_head_bigrams</th>\n",
       "      <th>count_of_unique_head_bigrams</th>\n",
       "      <th>ratio_of_unique_head_bigrams</th>\n",
       "      <th>count_of_head_trigrams</th>\n",
       "      <th>count_of_unique_head_trigrams</th>\n",
       "      <th>ratio_of_unique_head_trigrams</th>\n",
       "      <th>count_of_body_unigrams</th>\n",
       "      <th>...</th>\n",
       "      <th>count_of_unique_body_trigrams</th>\n",
       "      <th>ratio_of_unique_body_trigrams</th>\n",
       "      <th>count_of_head_unigrams_in_body</th>\n",
       "      <th>ratio_of_head_unigrams_in_body</th>\n",
       "      <th>count_of_head_bigrams_in_body</th>\n",
       "      <th>ratio_of_head_bigrams_in_body</th>\n",
       "      <th>count_of_head_trigrams_in_body</th>\n",
       "      <th>ratio_of_head_trigrams_in_body</th>\n",
       "      <th>len_sent_head</th>\n",
       "      <th>len_sent_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>258</td>\n",
       "      <td>...</td>\n",
       "      <td>249</td>\n",
       "      <td>0.972656</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>297</td>\n",
       "      <td>...</td>\n",
       "      <td>295</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296</td>\n",
       "      <td>...</td>\n",
       "      <td>259</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200</td>\n",
       "      <td>...</td>\n",
       "      <td>197</td>\n",
       "      <td>0.994949</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>367</td>\n",
       "      <td>...</td>\n",
       "      <td>327</td>\n",
       "      <td>0.895890</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3383</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>283</td>\n",
       "      <td>...</td>\n",
       "      <td>281</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3384</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>127</td>\n",
       "      <td>...</td>\n",
       "      <td>125</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3385</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101</td>\n",
       "      <td>...</td>\n",
       "      <td>98</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3386</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>156</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3387</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>202</td>\n",
       "      <td>...</td>\n",
       "      <td>200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3388 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      count_of_head_unigrams  count_of_unique_head_unigrams  \\\n",
       "0                          8                              8   \n",
       "1                          5                              5   \n",
       "2                         13                             13   \n",
       "3                          8                              8   \n",
       "4                          8                              8   \n",
       "...                      ...                            ...   \n",
       "3383                       6                              6   \n",
       "3384                       8                              8   \n",
       "3385                      10                             10   \n",
       "3386                      10                             10   \n",
       "3387                       9                              9   \n",
       "\n",
       "      ratio_of_unique_head_unigrams  count_of_head_bigrams  \\\n",
       "0                               1.0                      7   \n",
       "1                               1.0                      4   \n",
       "2                               1.0                     12   \n",
       "3                               1.0                      7   \n",
       "4                               1.0                      7   \n",
       "...                             ...                    ...   \n",
       "3383                            1.0                      5   \n",
       "3384                            1.0                      7   \n",
       "3385                            1.0                      9   \n",
       "3386                            1.0                      9   \n",
       "3387                            1.0                      8   \n",
       "\n",
       "      count_of_unique_head_bigrams  ratio_of_unique_head_bigrams  \\\n",
       "0                                7                           1.0   \n",
       "1                                4                           1.0   \n",
       "2                               12                           1.0   \n",
       "3                                7                           1.0   \n",
       "4                                7                           1.0   \n",
       "...                            ...                           ...   \n",
       "3383                             5                           1.0   \n",
       "3384                             7                           1.0   \n",
       "3385                             9                           1.0   \n",
       "3386                             9                           1.0   \n",
       "3387                             8                           1.0   \n",
       "\n",
       "      count_of_head_trigrams  count_of_unique_head_trigrams  \\\n",
       "0                          6                              6   \n",
       "1                          3                              3   \n",
       "2                         11                             11   \n",
       "3                          6                              6   \n",
       "4                          6                              6   \n",
       "...                      ...                            ...   \n",
       "3383                       4                              4   \n",
       "3384                       6                              6   \n",
       "3385                       8                              8   \n",
       "3386                       8                              8   \n",
       "3387                       7                              7   \n",
       "\n",
       "      ratio_of_unique_head_trigrams  count_of_body_unigrams  ...  \\\n",
       "0                               1.0                     258  ...   \n",
       "1                               1.0                     297  ...   \n",
       "2                               1.0                     296  ...   \n",
       "3                               1.0                     200  ...   \n",
       "4                               1.0                     367  ...   \n",
       "...                             ...                     ...  ...   \n",
       "3383                            1.0                     283  ...   \n",
       "3384                            1.0                     127  ...   \n",
       "3385                            1.0                     101  ...   \n",
       "3386                            1.0                     158  ...   \n",
       "3387                            1.0                     202  ...   \n",
       "\n",
       "      count_of_unique_body_trigrams  ratio_of_unique_body_trigrams  \\\n",
       "0                               249                       0.972656   \n",
       "1                               295                       1.000000   \n",
       "2                               259                       0.880952   \n",
       "3                               197                       0.994949   \n",
       "4                               327                       0.895890   \n",
       "...                             ...                            ...   \n",
       "3383                            281                       1.000000   \n",
       "3384                            125                       1.000000   \n",
       "3385                             98                       0.989899   \n",
       "3386                            156                       1.000000   \n",
       "3387                            200                       1.000000   \n",
       "\n",
       "      count_of_head_unigrams_in_body  ratio_of_head_unigrams_in_body  \\\n",
       "0                                7.0                        0.875000   \n",
       "1                                3.0                        0.600000   \n",
       "2                               13.0                        1.000000   \n",
       "3                                5.0                        0.625000   \n",
       "4                                7.0                        0.875000   \n",
       "...                              ...                             ...   \n",
       "3383                             6.0                        1.000000   \n",
       "3384                             3.0                        0.375000   \n",
       "3385                             4.0                        0.400000   \n",
       "3386                             8.0                        0.800000   \n",
       "3387                             6.0                        0.666667   \n",
       "\n",
       "      count_of_head_bigrams_in_body  ratio_of_head_bigrams_in_body  \\\n",
       "0                               4.0                       0.571429   \n",
       "1                               1.0                       0.250000   \n",
       "2                              11.0                       0.916667   \n",
       "3                               1.0                       0.142857   \n",
       "4                               5.0                       0.714286   \n",
       "...                             ...                            ...   \n",
       "3383                            2.0                       0.400000   \n",
       "3384                            0.0                       0.000000   \n",
       "3385                            0.0                       0.000000   \n",
       "3386                            2.0                       0.222222   \n",
       "3387                            2.0                       0.250000   \n",
       "\n",
       "      count_of_head_trigrams_in_body  ratio_of_head_trigrams_in_body  \\\n",
       "0                                1.0                        0.166667   \n",
       "1                                0.0                        0.000000   \n",
       "2                                9.0                        0.818182   \n",
       "3                                0.0                        0.000000   \n",
       "4                                3.0                        0.500000   \n",
       "...                              ...                             ...   \n",
       "3383                             0.0                        0.000000   \n",
       "3384                             0.0                        0.000000   \n",
       "3385                             0.0                        0.000000   \n",
       "3386                             0.0                        0.000000   \n",
       "3387                             0.0                        0.000000   \n",
       "\n",
       "      len_sent_head  len_sent_body  \n",
       "0                 1             17  \n",
       "1                 1             35  \n",
       "2                 1             20  \n",
       "3                 1             11  \n",
       "4                 1             26  \n",
       "...             ...            ...  \n",
       "3383              1             23  \n",
       "3384              1             10  \n",
       "3385              1              7  \n",
       "3386              1             22  \n",
       "3387              1             19  \n",
       "\n",
       "[3388 rows x 26 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_training_datasets[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Simple ML Magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate features + respective training sets\n",
    "# simple test with 75% of the dataset used for training, 25% used for testing\n",
    "partition = int(0.75 * len(training_data))\n",
    "\n",
    "features = [CountFeatureGenerator(), TfidfFeatureGenerator()]\n",
    "\n",
    "for feature in features:\n",
    "    feature.process(training_data.copy())\n",
    "    \n",
    "tree_training_datasets = []\n",
    "for feature in features:\n",
    "    for training_dataset in feature.read():\n",
    "        x_train = training_dataset[:partition]\n",
    "        x_test = training_dataset[partition:]\n",
    "        tree_training_datasets.append((feature._name, x_train, x_test))\n",
    "\n",
    "y_train = training_data['Stance'].values[:partition]\n",
    "y_test = training_data['Stance'].values[partition:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score on countFeatureGenerator: 0.9111570247933884\n",
      "f1 score on tfidfFeatureGenerator: 0.8948211425520555\n",
      "f1 score on tfidfFeatureGenerator: 0.8947087119187601\n",
      "f1 score on tfidfFeatureGenerator: 0.8789873417721519\n"
     ]
    }
   ],
   "source": [
    "# Run a GradientBoostingClassifier on each feature and plot their Precision-Recall Curves and note their f1 scores\n",
    "for (feature, x_train, x_test) in tree_training_datasets:\n",
    "    clf = GradientBoostingClassifier(n_estimators=300, max_depth=5).fit(x_train, y_train)\n",
    "    guess = clf.predict(x_test)\n",
    "    score = f1_score(y_test, guess)\n",
    "    print(f\"f1 score on {feature}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all tables into one giant table, and run a few ensemble methods on it\n",
    "master_df = pd.DataFrame()\n",
    "for (_, x_train, x_test) in tree_training_datasets:\n",
    "    master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
