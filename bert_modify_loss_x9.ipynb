{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from torch.nn import CrossEntropyLoss, MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "Using GPU: TITAN Xp\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda:1\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('Using GPU:', torch.cuda.get_device_name(1))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [1.0, 9.0, 9.0, 9.0]\n",
    "class_weights = torch.FloatTensor(weights).cuda(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassificationWeighted(BertForSequenceClassification):\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        weights=class_weights\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`, defaults to :obj:`None`):\n",
    "            Labels for computing the sequence classification/regression loss.\n",
    "            Indices should be in :obj:`[0, ..., config.num_labels - 1]`.\n",
    "            If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
    "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "    Returns:\n",
    "        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.BertConfig`) and inputs:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`label` is provided):\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.num_labels)`):\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n",
    "            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    Examples::\n",
    "        from transformers import BertTokenizer, BertForSequenceClassification\n",
    "        import torch\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss, logits = outputs[:2]\n",
    "        \"\"\"\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                #  We are doing regression\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss(weight=weights)\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle(\"data/jerome_request_train.pkl\")\n",
    "df = pd.get_dummies(df, columns=['stance'])\n",
    "df['label'] = df[['stance_unrelated','stance_agree','stance_disagree','stance_discuss']].values.tolist()\n",
    "df['label'] = df['label'].apply(lambda x: x.index(1))\n",
    "# grab np values\n",
    "headlines = df['head'].values\n",
    "bodies = df['body'].values\n",
    "labels = df['label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.read_pickle(\"data/jerome_request_test.pkl\")\n",
    "df_val = pd.get_dummies(df_val, columns=['stance'])\n",
    "df_val['label'] = df_val[['stance_unrelated','stance_agree','stance_disagree','stance_discuss']].values.tolist()\n",
    "df_val['label'] = df_val['label'].apply(lambda x: x.index(1))\n",
    "# grab np values\n",
    "headlines_val = df_val['head'].values\n",
    "bodies_val = df_val['body'].values\n",
    "labels_val = df_val['label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Hundreds of Palestinians flee floods in Gaza as Israel opens dams Hundreds of Palestinians were evacuated from their homes Sunday morning after Israeli authorities opened a number of dams near the border, flooding the Gaza Valley in the wake of a recent severe winter storm.\n",
      "\n",
      "The Gaza Ministry of Interior said in a statement that civil defense services and teams from the Ministry of Public Works had evacuated more than 80 families from both sides of the Gaza Valley (Wadi Gaza) after their homes flooded as water levels reached more than three meters.\n",
      "\n",
      "Gaza has experienced flooding in recent days amid a major storm that saw temperatures drop and frigid rain pour down.\n",
      "\n",
      "The storm displaced dozens and caused hardship for tens of thousands, including many of the approximately 110,000 Palestinians left homeless by Israel's assault over summer.\n",
      "\n",
      "The suffering is compounded by the fact that Israel has maintained a complete siege over Gaza for the last eight years, severely limiting electricity and the availability of fuel for generators. It has also prevented the displaced from rebuilding their homes, as construction materials are largely banned from entering.\n",
      "\n",
      "Gaza civil defense services spokesman Muhammad al-Midana warned that further harm could be caused if Israel opens up more dams in the area, noting that water is currently flowing at a high speed from the Israel border through the valley and into the Mediterranean sea.\n",
      "\n",
      "Evacuated families have been sent to shelters sponsored by UNRWA, the UN agency for Palestinian refugees, in al-Bureij refugee camp and in al-Zahra neighborhood in the central Gaza Strip.\n",
      "\n",
      "The Gaza Valley (Wadi Gaza) is a wetland located in the central Gaza Strip between al-Nuseirat refugee camp and al-Moghraqa. It is called HaBesor in Hebrew, and it flows from two streams -- one whose source runs from near Beersheba, and the other from near Hebron.\n",
      "\n",
      "Israeli dams on the river to collect rainwater have dried up the wetlands inside Gaza, and destroyed the only source of surface water in the area.\n",
      "\n",
      "Locals have continued to use it to dispose of their waste for lack of other ways to do so, however, creating an environmental hazard.\n",
      "\n",
      "This is not the first time Israeli authorities have opened the Gaza Valley dams.\n",
      "\n",
      "In Dec. 2013, Israeli authorities also opened the dams amid heavy flooding in the Gaza Strip. The resulting floods damaged dozens of homes and forces many families in the area from their homes.\n",
      "\n",
      "In 2010, the dams were opened as well, forcing 100 families from their homes. At the time civil defense services said that they had managed to save seven people who had been at risk of drowning.\n",
      "Token IDs: tensor([  101,  5606,  1997, 21524, 10574, 14295,  1999, 14474,  2004,  3956,\n",
      "         7480, 17278,   102,  5606,  1997, 21524,  2020, 13377,  2013,  2037,\n",
      "         5014,  4465,  2851,  2044,  5611,  4614,  2441,  1037,  2193,  1997,\n",
      "        17278,  2379,  1996,  3675,  1010,  9451,  1996, 14474,  3028,  1999,\n",
      "         1996,  5256,  1997,  1037,  3522,  5729,  3467,  4040,  1012,  1996,\n",
      "        14474,  3757,  1997,  4592,  2056,  1999,  1037,  4861,  2008,  2942,\n",
      "         3639,  2578,  1998,  2780,  2013,  1996,  3757,  1997,  2270,  2573,\n",
      "         2018, 13377,  2062,  2084,  3770,  2945,  2013,  2119,  3903,  1997,\n",
      "         1996, 14474,  3028,  1006, 28380, 14474,  1007,  2044,  2037,  5014,\n",
      "        10361,  2004,  2300,  3798,  2584,  2062,  2084,  2093,  5563,  1012,\n",
      "        14474,  2038,  5281,  9451,  1999,  3522,  2420, 13463,  1037,  2350,\n",
      "         4040,  2008,  2387,  7715,  4530,  1998, 10424,  8004,  3593,  4542,\n",
      "        10364,  2091,  1012,  1996,  4040, 12936,  9877,   102])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# tokenization\n",
    "input_ids = []\n",
    "#attention_masks = []\n",
    "\n",
    "for i in range(len(headlines)):\n",
    "    encoded_dict = tokenizer.encode(\n",
    "                        text=headlines[i],                      # headline to encode.\n",
    "                        text_pair=bodies[i],\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 128,\n",
    "                        pad_to_max_length=True,   # Pad & truncate all sentences.\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.   \n",
    "    #input_ids.append(encoded_dict['input_ids'])\n",
    "    input_ids.append(encoded_dict)\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    #attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "#attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', headlines[0], bodies[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Police find mass graves with at least '15 bodies' near Mexico town where 43 students disappeared after police clash Danny Boyle is directing the untitled film\n",
      "\n",
      "Seth Rogen is being eyed to play Apple co-founder Steve Wozniak in Sony’s Steve Jobs biopic.\n",
      "\n",
      "Danny Boyle is directing the untitled film, based on Walter Isaacson's book and adapted by Aaron Sorkin, which is one of the most anticipated biopics in recent years.\n",
      "\n",
      "Negotiations have not yet begun, and it’s not even clear if Rogen has an official offer, but the producers — Scott Rudin, Guymon Casady and Mark Gordon — have set their sights on the talent and are in talks.\n",
      "\n",
      "Of course, this may all be for naught as Christian Bale, the actor who is to play Jobs, is still in the midst of closing his deal. Sources say that dealmaking process is in a sensitive stage.\n",
      "\n",
      "Insiders say Boyle will is flying to Los Angeles to meet with actress to play one of the female leads, an assistant to Jobs. Insiders say that Jessica Chastain is one of the actresses on the meeting list.\n",
      "\n",
      "Wozniak, known as \"Woz,\" co-founded Apple with Jobs and Ronald Wayne. He first met Jobs when they worked at Atari and later was responsible for creating the early Apple computers.\n",
      "Token IDs: tensor([  101,  2610,  2424,  3742,  9729,  2007,  2012,  2560,  1005,  2321,\n",
      "         4230,  1005,  2379,  3290,  2237,  2073,  4724,  2493,  5419,  2044,\n",
      "         2610, 13249,   102,  6266, 16694,  2003,  9855,  1996, 24819,  2143,\n",
      "         6662, 20996,  6914,  2003,  2108,  7168,  2000,  2377,  6207,  2522,\n",
      "         1011,  3910,  3889, 24185,  2480,  6200,  2243,  1999,  8412,  1521,\n",
      "         1055,  3889,  5841, 16012, 24330,  1012,  6266, 16694,  2003,  9855,\n",
      "         1996, 24819,  2143,  1010,  2241,  2006,  4787,  7527,  3385,  1005,\n",
      "         1055,  2338,  1998,  5967,  2011,  7158,  2061, 26891,  1010,  2029,\n",
      "         2003,  2028,  1997,  1996,  2087, 11436, 16012, 24330,  2015,  1999,\n",
      "         3522,  2086,  1012,  7776,  2031,  2025,  2664,  5625,  1010,  1998,\n",
      "         2009,  1521,  1055,  2025,  2130,  3154,  2065, 20996,  6914,  2038,\n",
      "         2019,  2880,  3749,  1010,  2021,  1996,  6443,  1517,  3660, 21766,\n",
      "         8718,  1010,  3124,  8202, 14124,  5149,  1998,   102])\n"
     ]
    }
   ],
   "source": [
    "# tokenization\n",
    "input_ids_val = []\n",
    "#attention_masks = []\n",
    "\n",
    "for i in range(len(headlines_val)):\n",
    "    encoded_dict = tokenizer.encode(\n",
    "                        text=headlines_val[i],                      # headline to encode.\n",
    "                        text_pair=bodies_val[i],\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 128,\n",
    "                        pad_to_max_length=True,   # Pad & truncate all sentences.\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.   \n",
    "    #input_ids.append(encoded_dict['input_ids'])\n",
    "    input_ids_val.append(encoded_dict)\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    #attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids_val = torch.cat(input_ids_val, dim=0)\n",
    "#attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels_val = torch.tensor(labels_val)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', headlines_val[0], bodies_val[0])\n",
    "print('Token IDs:', input_ids_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45,437 training samples\n",
      "4,535 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "train_dataset = TensorDataset(input_ids, labels)\n",
    "val_dataset = TensorDataset(input_ids_val, labels_val)\n",
    "train_size = len(train_dataset)\n",
    "val_size = len(val_dataset)\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassificationWeighted(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AdamW, BertConfig\n",
    "\n",
    "model = BertForSequenceClassificationWeighted.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 4, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "model.cuda(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 24\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate fnc score and max fnc score\n",
    "def fnc_score(preds, labels):\n",
    "    y_pred = np.argmax(preds, axis=1).flatten()\n",
    "    y_true = labels.flatten()\n",
    "\n",
    "    # compute max_score = 0.25*unrelated + (agree+disagree+discuss)\n",
    "    total_count = torch.tensor(len(y_true))\n",
    "    unrelated_count = torch.sum(y_true == 0)\n",
    "    related_count = total_count - unrelated_count\n",
    "    max_score = 0.25 * unrelated_count + related_count\n",
    "\n",
    "    # compute score\n",
    "    unrelated_pred = y_pred == 0\n",
    "    unrelated_true = y_true == 0\n",
    "    correct_unrelated_count = torch.sum(unrelated_pred & unrelated_true)\n",
    "    correct_unrelated_count_score = 0.25 * correct_unrelated_count\n",
    "\n",
    "    is_related_mask = ~unrelated_pred\n",
    "    is_correct_mask = y_true == y_pred\n",
    "\n",
    "    combined_mask_correct_related = is_related_mask & is_correct_mask\n",
    "    correct_related_count = torch.sum(combined_mask_correct_related)\n",
    "    correct_related_count_score = 1.0 * correct_related_count\n",
    "\n",
    "    is_related_true_mask = ~unrelated_true\n",
    "    combined_mask_related = is_related_mask & is_related_true_mask\n",
    "    combined_mask_incorrect_related = combined_mask_related & ~combined_mask_correct_related\n",
    "    incorrect_related_count = torch.sum(combined_mask_incorrect_related)\n",
    "    incorrect_related_count_score = 0.25 * incorrect_related_count\n",
    "\n",
    "    score = correct_unrelated_count_score + correct_related_count_score + incorrect_related_count_score\n",
    "    return score, max_score\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return torch.sum(pred_flat == labels_flat).double() / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "def train(model, epochs, train_dataloader, validation_dataloader, device):\n",
    "    training_stats = []\n",
    "\n",
    "    # Measure the total training time for the whole run.\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "\n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "\n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "\n",
    "        # Put the model into training mode. Don't be mislead--the call to \n",
    "        # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "        # `dropout` and `batchnorm` layers behave differently during training\n",
    "        # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Progress update every 100 batches.\n",
    "            if step % 100 == 0 and step != 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "\n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "            # `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            #b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[1].to(device)\n",
    "\n",
    "            # Always clear any previously calculated gradients before performing a\n",
    "            # backward pass. PyTorch doesn't do this automatically because \n",
    "            # accumulating the gradients is \"convenient while training RNNs\". \n",
    "            # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "            model.zero_grad()        \n",
    "\n",
    "            # Perform a forward pass (evaluate the model on this training batch).\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # It returns different numbers of parameters depending on what arguments\n",
    "            # arge given and what flags are set. For our useage here, it returns\n",
    "            # the loss (because we provided labels) and the \"logits\"--the model\n",
    "            # outputs prior to activation.\n",
    "            loss, logits = model(input_ids=b_input_ids,\n",
    "                                 labels=b_labels)\n",
    "\n",
    "            # Accumulate the training loss over all of the batches so that we can\n",
    "            # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "            # single value; the `.item()` function just returns the Python value \n",
    "            # from the tensor.\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and take a step using the computed gradient.\n",
    "            # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "            # modified based on their gradients, the learning rate, etc.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the learning rate.\n",
    "            scheduler.step()\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "\n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epoch took: {:}\".format(training_time))\n",
    "\n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_fnc_score = 0\n",
    "        total_eval_max_fnc_score = 0\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "\n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "            # the `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "            b_input_ids = batch[0].to(device)\n",
    "            #b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[1].to(device)\n",
    "\n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():        \n",
    "\n",
    "                # Forward pass, calculate logit predictions.\n",
    "                # token_type_ids is the same as the \"segment ids\", which \n",
    "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "                # The documentation for this `model` function is here: \n",
    "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "                # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "                # values prior to applying an activation function like the softmax.\n",
    "                (loss, logits) = model(b_input_ids, \n",
    "                                       token_type_ids=None, \n",
    "                                       labels=b_labels)\n",
    "\n",
    "            # Accumulate the validation loss.\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu()\n",
    "            label_ids = b_labels.to('cpu')\n",
    "\n",
    "            # Calculate the accuracy for this batch of test sentences, and\n",
    "            # accumulate it over all batches.\n",
    "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "            # Calculate FNC score, max FNC score for this batch, and accumulate\n",
    "            b_fnc_score, b_max_fnc_score = fnc_score(logits, label_ids)\n",
    "            total_eval_fnc_score += b_fnc_score\n",
    "            total_eval_max_fnc_score += b_max_fnc_score\n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        avg_val_accuracy = (total_eval_accuracy / len(validation_dataloader)).item()\n",
    "        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "        # Report fnc scores over validation run\n",
    "        total_eval_fnc_score = total_eval_fnc_score.item()\n",
    "        total_eval_max_fnc_score = total_eval_max_fnc_score.item()\n",
    "        relative_score = total_eval_fnc_score / total_eval_max_fnc_score\n",
    "        print(\"  FNC Score: {0:.2f}\".format(total_eval_fnc_score))\n",
    "        print(\"  Maximum Possible FNC Score: {0:.2f}\".format(total_eval_max_fnc_score))\n",
    "        print(\"  Relative FNC Score: {0:.2f}\".format(total_eval_fnc_score / total_eval_max_fnc_score))\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "        # Measure how long the validation run took.\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "        print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch_i + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Valid. Accur.': avg_val_accuracy,\n",
    "                'Training Time': training_time,\n",
    "                'Validation Time': validation_time,\n",
    "                'Valid. FNC Score': total_eval_fnc_score,\n",
    "                'Valid. Max FNC Score': total_eval_max_fnc_score,\n",
    "                'Relative FNC Score': relative_score\n",
    "            }\n",
    "        )\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "    return training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 24 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1587428094786/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of  1,420.    Elapsed: 0:00:32.\n",
      "  Batch   200  of  1,420.    Elapsed: 0:01:05.\n",
      "  Batch   300  of  1,420.    Elapsed: 0:01:38.\n",
      "  Batch   400  of  1,420.    Elapsed: 0:02:12.\n",
      "  Batch   500  of  1,420.    Elapsed: 0:02:45.\n",
      "  Batch   600  of  1,420.    Elapsed: 0:03:18.\n",
      "  Batch   700  of  1,420.    Elapsed: 0:03:52.\n",
      "  Batch   800  of  1,420.    Elapsed: 0:04:25.\n",
      "  Batch   900  of  1,420.    Elapsed: 0:04:59.\n",
      "  Batch 1,000  of  1,420.    Elapsed: 0:05:33.\n",
      "  Batch 1,100  of  1,420.    Elapsed: 0:06:06.\n",
      "  Batch 1,200  of  1,420.    Elapsed: 0:06:40.\n",
      "  Batch 1,300  of  1,420.    Elapsed: 0:07:13.\n",
      "  Batch 1,400  of  1,420.    Elapsed: 0:07:47.\n",
      "\n",
      "  Average training loss: 0.46\n",
      "  Training epoch took: 0:07:54\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  FNC Score: 1849.75\n",
      "  Maximum Possible FNC Score: 2061.50\n",
      "  Relative FNC Score: 0.90\n",
      "  Validation Loss: 0.54\n",
      "  Validation took: 0:00:15\n",
      "\n",
      "======== Epoch 2 / 24 ========\n",
      "Training...\n",
      "  Batch   100  of  1,420.    Elapsed: 0:00:34.\n",
      "  Batch   200  of  1,420.    Elapsed: 0:01:07.\n",
      "  Batch   300  of  1,420.    Elapsed: 0:01:41.\n",
      "  Batch   400  of  1,420.    Elapsed: 0:02:14.\n",
      "  Batch   500  of  1,420.    Elapsed: 0:02:48.\n",
      "  Batch   600  of  1,420.    Elapsed: 0:03:21.\n",
      "  Batch   700  of  1,420.    Elapsed: 0:03:55.\n",
      "  Batch   800  of  1,420.    Elapsed: 0:04:28.\n",
      "  Batch   900  of  1,420.    Elapsed: 0:05:02.\n",
      "  Batch 1,000  of  1,420.    Elapsed: 0:05:35.\n",
      "  Batch 1,100  of  1,420.    Elapsed: 0:06:09.\n",
      "  Batch 1,200  of  1,420.    Elapsed: 0:06:42.\n",
      "  Batch 1,300  of  1,420.    Elapsed: 0:07:16.\n",
      "  Batch 1,400  of  1,420.    Elapsed: 0:07:49.\n",
      "\n",
      "  Average training loss: 0.18\n",
      "  Training epoch took: 0:07:56\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.94\n",
      "  FNC Score: 1862.75\n",
      "  Maximum Possible FNC Score: 2061.50\n",
      "  Relative FNC Score: 0.90\n",
      "  Validation Loss: 0.72\n",
      "  Validation took: 0:00:15\n",
      "\n",
      "======== Epoch 3 / 24 ========\n",
      "Training...\n",
      "  Batch   100  of  1,420.    Elapsed: 0:00:34.\n",
      "  Batch   200  of  1,420.    Elapsed: 0:01:07.\n",
      "  Batch   300  of  1,420.    Elapsed: 0:01:41.\n",
      "  Batch   400  of  1,420.    Elapsed: 0:02:14.\n",
      "  Batch   500  of  1,420.    Elapsed: 0:02:47.\n",
      "  Batch   600  of  1,420.    Elapsed: 0:03:21.\n",
      "  Batch   700  of  1,420.    Elapsed: 0:03:54.\n",
      "  Batch   800  of  1,420.    Elapsed: 0:04:28.\n",
      "  Batch   900  of  1,420.    Elapsed: 0:05:01.\n",
      "  Batch 1,000  of  1,420.    Elapsed: 0:05:34.\n",
      "  Batch 1,100  of  1,420.    Elapsed: 0:06:08.\n",
      "  Batch 1,200  of  1,420.    Elapsed: 0:06:41.\n",
      "  Batch 1,300  of  1,420.    Elapsed: 0:07:15.\n",
      "  Batch 1,400  of  1,420.    Elapsed: 0:07:48.\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epoch took: 0:07:55\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.94\n",
      "  FNC Score: 1855.00\n",
      "  Maximum Possible FNC Score: 2061.50\n",
      "  Relative FNC Score: 0.90\n",
      "  Validation Loss: 0.93\n",
      "  Validation took: 0:00:15\n",
      "\n",
      "======== Epoch 4 / 24 ========\n",
      "Training...\n",
      "  Batch   100  of  1,420.    Elapsed: 0:00:33.\n",
      "  Batch   200  of  1,420.    Elapsed: 0:01:07.\n",
      "  Batch   300  of  1,420.    Elapsed: 0:01:40.\n",
      "  Batch   400  of  1,420.    Elapsed: 0:02:14.\n",
      "  Batch   500  of  1,420.    Elapsed: 0:02:47.\n",
      "  Batch   600  of  1,420.    Elapsed: 0:03:20.\n",
      "  Batch   700  of  1,420.    Elapsed: 0:03:54.\n",
      "  Batch   800  of  1,420.    Elapsed: 0:04:27.\n",
      "  Batch   900  of  1,420.    Elapsed: 0:05:00.\n",
      "  Batch 1,000  of  1,420.    Elapsed: 0:05:34.\n",
      "  Batch 1,100  of  1,420.    Elapsed: 0:06:07.\n",
      "  Batch 1,200  of  1,420.    Elapsed: 0:06:40.\n",
      "  Batch 1,300  of  1,420.    Elapsed: 0:07:14.\n",
      "  Batch 1,400  of  1,420.    Elapsed: 0:07:47.\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epoch took: 0:07:54\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.94\n",
      "  FNC Score: 1865.00\n",
      "  Maximum Possible FNC Score: 2061.50\n",
      "  Relative FNC Score: 0.90\n",
      "  Validation Loss: 0.94\n",
      "  Validation took: 0:00:15\n",
      "\n",
      "======== Epoch 5 / 24 ========\n",
      "Training...\n",
      "  Batch   100  of  1,420.    Elapsed: 0:00:33.\n",
      "  Batch   200  of  1,420.    Elapsed: 0:01:07.\n",
      "  Batch   300  of  1,420.    Elapsed: 0:01:40.\n",
      "  Batch   400  of  1,420.    Elapsed: 0:02:14.\n",
      "  Batch   500  of  1,420.    Elapsed: 0:02:47.\n",
      "  Batch   600  of  1,420.    Elapsed: 0:03:20.\n",
      "  Batch   700  of  1,420.    Elapsed: 0:03:54.\n",
      "  Batch   800  of  1,420.    Elapsed: 0:04:27.\n",
      "  Batch   900  of  1,420.    Elapsed: 0:05:01.\n",
      "  Batch 1,000  of  1,420.    Elapsed: 0:05:34.\n",
      "  Batch 1,100  of  1,420.    Elapsed: 0:06:07.\n",
      "  Batch 1,200  of  1,420.    Elapsed: 0:06:41.\n",
      "  Batch 1,300  of  1,420.    Elapsed: 0:07:15.\n",
      "  Batch 1,400  of  1,420.    Elapsed: 0:07:48.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:07:55\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.94\n",
      "  FNC Score: 1871.25\n",
      "  Maximum Possible FNC Score: 2061.50\n",
      "  Relative FNC Score: 0.91\n",
      "  Validation Loss: 1.01\n",
      "  Validation took: 0:00:15\n",
      "\n",
      "======== Epoch 6 / 24 ========\n",
      "Training...\n",
      "  Batch   100  of  1,420.    Elapsed: 0:00:34.\n",
      "  Batch   200  of  1,420.    Elapsed: 0:01:07.\n",
      "  Batch   300  of  1,420.    Elapsed: 0:01:40.\n",
      "  Batch   400  of  1,420.    Elapsed: 0:02:14.\n",
      "  Batch   500  of  1,420.    Elapsed: 0:02:47.\n",
      "  Batch   600  of  1,420.    Elapsed: 0:03:20.\n",
      "  Batch   700  of  1,420.    Elapsed: 0:03:53.\n",
      "  Batch   800  of  1,420.    Elapsed: 0:04:26.\n",
      "  Batch   900  of  1,420.    Elapsed: 0:05:00.\n",
      "  Batch 1,000  of  1,420.    Elapsed: 0:05:33.\n",
      "  Batch 1,100  of  1,420.    Elapsed: 0:06:07.\n",
      "  Batch 1,200  of  1,420.    Elapsed: 0:06:40.\n",
      "  Batch 1,300  of  1,420.    Elapsed: 0:07:14.\n",
      "  Batch 1,400  of  1,420.    Elapsed: 0:07:47.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 0:07:54\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "  FNC Score: 1879.50\n",
      "  Maximum Possible FNC Score: 2061.50\n",
      "  Relative FNC Score: 0.91\n",
      "  Validation Loss: 1.08\n",
      "  Validation took: 0:00:15\n",
      "\n",
      "======== Epoch 7 / 24 ========\n",
      "Training...\n",
      "  Batch   100  of  1,420.    Elapsed: 0:00:34.\n",
      "  Batch   200  of  1,420.    Elapsed: 0:01:07.\n",
      "  Batch   300  of  1,420.    Elapsed: 0:01:41.\n",
      "  Batch   400  of  1,420.    Elapsed: 0:02:14.\n",
      "  Batch   500  of  1,420.    Elapsed: 0:02:48.\n",
      "  Batch   600  of  1,420.    Elapsed: 0:03:21.\n",
      "  Batch   700  of  1,420.    Elapsed: 0:03:55.\n",
      "  Batch   800  of  1,420.    Elapsed: 0:04:28.\n",
      "  Batch   900  of  1,420.    Elapsed: 0:05:02.\n",
      "  Batch 1,000  of  1,420.    Elapsed: 0:05:35.\n",
      "  Batch 1,100  of  1,420.    Elapsed: 0:06:08.\n",
      "  Batch 1,200  of  1,420.    Elapsed: 0:06:42.\n",
      "  Batch 1,300  of  1,420.    Elapsed: 0:07:16.\n",
      "  Batch 1,400  of  1,420.    Elapsed: 0:07:50.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 0:07:57\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.94\n",
      "  FNC Score: 1873.00\n",
      "  Maximum Possible FNC Score: 2061.50\n",
      "  Relative FNC Score: 0.91\n",
      "  Validation Loss: 1.12\n",
      "  Validation took: 0:00:15\n",
      "\n",
      "======== Epoch 8 / 24 ========\n",
      "Training...\n",
      "  Batch   100  of  1,420.    Elapsed: 0:00:34.\n",
      "  Batch   200  of  1,420.    Elapsed: 0:01:08.\n",
      "  Batch   300  of  1,420.    Elapsed: 0:01:41.\n",
      "  Batch   400  of  1,420.    Elapsed: 0:02:15.\n",
      "  Batch   500  of  1,420.    Elapsed: 0:02:48.\n",
      "  Batch   600  of  1,420.    Elapsed: 0:03:22.\n",
      "  Batch   700  of  1,420.    Elapsed: 0:03:55.\n",
      "  Batch   800  of  1,420.    Elapsed: 0:04:29.\n",
      "  Batch   900  of  1,420.    Elapsed: 0:05:02.\n",
      "  Batch 1,000  of  1,420.    Elapsed: 0:05:36.\n",
      "  Batch 1,100  of  1,420.    Elapsed: 0:06:09.\n",
      "  Batch 1,200  of  1,420.    Elapsed: 0:06:43.\n",
      "  Batch 1,300  of  1,420.    Elapsed: 0:07:16.\n",
      "  Batch 1,400  of  1,420.    Elapsed: 0:07:50.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:07:57\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "  FNC Score: 1876.50\n",
      "  Maximum Possible FNC Score: 2061.50\n",
      "  Relative FNC Score: 0.91\n",
      "  Validation Loss: 1.21\n",
      "  Validation took: 0:00:15\n",
      "\n",
      "======== Epoch 9 / 24 ========\n",
      "Training...\n",
      "  Batch   100  of  1,420.    Elapsed: 0:00:34.\n",
      "  Batch   200  of  1,420.    Elapsed: 0:01:07.\n",
      "  Batch   300  of  1,420.    Elapsed: 0:01:41.\n",
      "  Batch   400  of  1,420.    Elapsed: 0:02:14.\n",
      "  Batch   500  of  1,420.    Elapsed: 0:02:48.\n",
      "  Batch   600  of  1,420.    Elapsed: 0:03:21.\n",
      "  Batch   700  of  1,420.    Elapsed: 0:03:54.\n",
      "  Batch   800  of  1,420.    Elapsed: 0:04:28.\n",
      "  Batch   900  of  1,420.    Elapsed: 0:05:01.\n",
      "  Batch 1,000  of  1,420.    Elapsed: 0:05:35.\n",
      "  Batch 1,100  of  1,420.    Elapsed: 0:06:08.\n",
      "  Batch 1,200  of  1,420.    Elapsed: 0:06:42.\n",
      "  Batch 1,300  of  1,420.    Elapsed: 0:07:15.\n",
      "  Batch 1,400  of  1,420.    Elapsed: 0:07:48.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:07:55\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.95\n",
      "  FNC Score: 1903.25\n",
      "  Maximum Possible FNC Score: 2061.50\n",
      "  Relative FNC Score: 0.92\n",
      "  Validation Loss: 0.84\n",
      "  Validation took: 0:00:15\n",
      "\n",
      "======== Epoch 10 / 24 ========\n",
      "Training...\n",
      "  Batch   100  of  1,420.    Elapsed: 0:00:33.\n",
      "  Batch   200  of  1,420.    Elapsed: 0:01:07.\n",
      "  Batch   300  of  1,420.    Elapsed: 0:01:40.\n",
      "  Batch   400  of  1,420.    Elapsed: 0:02:13.\n",
      "  Batch   500  of  1,420.    Elapsed: 0:02:46.\n",
      "  Batch   600  of  1,420.    Elapsed: 0:03:19.\n",
      "  Batch   700  of  1,420.    Elapsed: 0:03:53.\n",
      "  Batch   800  of  1,420.    Elapsed: 0:04:26.\n",
      "  Batch   900  of  1,420.    Elapsed: 0:04:59.\n",
      "  Batch 1,000  of  1,420.    Elapsed: 0:05:32.\n",
      "  Batch 1,100  of  1,420.    Elapsed: 0:06:06.\n",
      "  Batch 1,200  of  1,420.    Elapsed: 0:06:40.\n",
      "  Batch 1,300  of  1,420.    Elapsed: 0:07:13.\n",
      "  Batch 1,400  of  1,420.    Elapsed: 0:07:46.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:07:53\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "  FNC Score: 1891.00\n",
      "  Maximum Possible FNC Score: 2061.50\n",
      "  Relative FNC Score: 0.92\n",
      "  Validation Loss: 1.21\n",
      "  Validation took: 0:00:15\n",
      "\n",
      "======== Epoch 11 / 24 ========\n",
      "Training...\n",
      "  Batch   100  of  1,420.    Elapsed: 0:00:34.\n",
      "  Batch   200  of  1,420.    Elapsed: 0:01:08.\n",
      "  Batch   300  of  1,420.    Elapsed: 0:01:41.\n",
      "  Batch   400  of  1,420.    Elapsed: 0:02:15.\n",
      "  Batch   500  of  1,420.    Elapsed: 0:02:49.\n",
      "  Batch   600  of  1,420.    Elapsed: 0:03:23.\n",
      "  Batch   700  of  1,420.    Elapsed: 0:03:56.\n",
      "  Batch   800  of  1,420.    Elapsed: 0:04:30.\n",
      "  Batch   900  of  1,420.    Elapsed: 0:05:04.\n",
      "  Batch 1,000  of  1,420.    Elapsed: 0:05:37.\n",
      "  Batch 1,100  of  1,420.    Elapsed: 0:06:11.\n",
      "  Batch 1,200  of  1,420.    Elapsed: 0:06:44.\n",
      "  Batch 1,300  of  1,420.    Elapsed: 0:07:18.\n",
      "  Batch 1,400  of  1,420.    Elapsed: 0:07:51.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:07:58\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.94\n",
      "  FNC Score: 1866.25\n",
      "  Maximum Possible FNC Score: 2061.50\n",
      "  Relative FNC Score: 0.91\n",
      "  Validation Loss: 1.35\n",
      "  Validation took: 0:00:15\n",
      "\n",
      "======== Epoch 12 / 24 ========\n",
      "Training...\n",
      "  Batch   100  of  1,420.    Elapsed: 0:00:34.\n",
      "  Batch   200  of  1,420.    Elapsed: 0:01:07.\n",
      "  Batch   300  of  1,420.    Elapsed: 0:01:40.\n",
      "  Batch   400  of  1,420.    Elapsed: 0:02:14.\n",
      "  Batch   500  of  1,420.    Elapsed: 0:02:47.\n",
      "  Batch   600  of  1,420.    Elapsed: 0:03:21.\n",
      "  Batch   700  of  1,420.    Elapsed: 0:03:54.\n",
      "  Batch   800  of  1,420.    Elapsed: 0:04:28.\n",
      "  Batch   900  of  1,420.    Elapsed: 0:05:01.\n",
      "  Batch 1,000  of  1,420.    Elapsed: 0:05:35.\n",
      "  Batch 1,100  of  1,420.    Elapsed: 0:06:08.\n",
      "  Batch 1,200  of  1,420.    Elapsed: 0:06:42.\n",
      "  Batch 1,300  of  1,420.    Elapsed: 0:07:15.\n",
      "  Batch 1,400  of  1,420.    Elapsed: 0:07:49.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:07:55\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "  FNC Score: 1882.75\n",
      "  Maximum Possible FNC Score: 2061.50\n",
      "  Relative FNC Score: 0.91\n",
      "  Validation Loss: 1.27\n",
      "  Validation took: 0:00:15\n",
      "\n",
      "======== Epoch 13 / 24 ========\n",
      "Training...\n",
      "  Batch   100  of  1,420.    Elapsed: 0:00:34.\n",
      "  Batch   200  of  1,420.    Elapsed: 0:01:07.\n",
      "  Batch   300  of  1,420.    Elapsed: 0:01:41.\n",
      "  Batch   400  of  1,420.    Elapsed: 0:02:14.\n",
      "  Batch   500  of  1,420.    Elapsed: 0:02:48.\n",
      "  Batch   600  of  1,420.    Elapsed: 0:03:21.\n",
      "  Batch   700  of  1,420.    Elapsed: 0:03:55.\n",
      "  Batch   800  of  1,420.    Elapsed: 0:04:28.\n",
      "  Batch   900  of  1,420.    Elapsed: 0:05:02.\n",
      "  Batch 1,000  of  1,420.    Elapsed: 0:05:35.\n",
      "  Batch 1,100  of  1,420.    Elapsed: 0:06:08.\n",
      "  Batch 1,200  of  1,420.    Elapsed: 0:06:41.\n",
      "  Batch 1,300  of  1,420.    Elapsed: 0:07:15.\n",
      "  Batch 1,400  of  1,420.    Elapsed: 0:07:48.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:07:54\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.95\n",
      "  FNC Score: 1889.50\n",
      "  Maximum Possible FNC Score: 2061.50\n",
      "  Relative FNC Score: 0.92\n",
      "  Validation Loss: 1.33\n",
      "  Validation took: 0:00:15\n",
      "\n",
      "======== Epoch 14 / 24 ========\n",
      "Training...\n",
      "  Batch   100  of  1,420.    Elapsed: 0:00:33.\n",
      "  Batch   200  of  1,420.    Elapsed: 0:01:07.\n",
      "  Batch   300  of  1,420.    Elapsed: 0:01:40.\n"
     ]
    }
   ],
   "source": [
    "training_stats = train(model, epochs, train_dataloader, validation_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = [x['epoch'] for x in training_stats]\n",
    "training_loss = [x['Training Loss'] for x in training_stats]\n",
    "validation_loss  = [x['Valid. Loss'] for x in training_stats]\n",
    "relative_validation_fnc = [x['Relative FNC Score'] for x in training_stats]\n",
    "validation_accuracy = [x['Valid. Accur.'] for x in training_stats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch, training_loss, label='train')\n",
    "plt.plot(epoch, validation_loss, label='validation')\n",
    "plt.title('Loss over training epochs')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.savefig('loss_128_w9.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch, relative_validation_fnc)\n",
    "plt.title('Evaluation on Validation Set (max 1.00)')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Relative FNC Score')\n",
    "plt.savefig('fnc_128_w9.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch, validation_accuracy)\n",
    "plt.title('Accuracy on Validation Set')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.savefig('accuracy_128_w9.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('training_stats_128_w9.pkl', 'wb') as f:\n",
    "    pickle.dump(training_stats, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_stats_128_w9.pkl', 'r') as f:\n",
    "    loaded = pickle.dump(training_stats, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
